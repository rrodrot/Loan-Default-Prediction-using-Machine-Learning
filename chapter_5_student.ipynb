{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Loan Prediction Machine Learning Model\n",
    "\n",
    "# Chapter 5 - Linear Classifiers\n",
    "\n",
    "### Load Data and Import Libraries\n",
    "\n",
    "- Notice that we have included two new modules from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df = pd.read_csv('../data/vehicle_loans_feat.csv', index_col='UNIQUEID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 - Train/Test Split\n",
    "\n",
    "For the rest of this chapter, we will work through the steps of creating a simple linear classifier using Logistic Regression\n",
    "\n",
    "First let's remind ourselves of the variables we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 233154 entries, 420825 to 630213\n",
      "Data columns (total 31 columns):\n",
      " #   Column                               Non-Null Count   Dtype  \n",
      "---  ------                               --------------   -----  \n",
      " 0   DISBURSED_AMOUNT                     233154 non-null  float64\n",
      " 1   ASSET_COST                           233154 non-null  float64\n",
      " 2   LTV                                  233154 non-null  float64\n",
      " 3   MANUFACTURER_ID                      233154 non-null  int64  \n",
      " 4   EMPLOYMENT_TYPE                      233154 non-null  object \n",
      " 5   STATE_ID                             233154 non-null  int64  \n",
      " 6   AADHAR_FLAG                          233154 non-null  int64  \n",
      " 7   PAN_FLAG                             233154 non-null  int64  \n",
      " 8   VOTERID_FLAG                         233154 non-null  int64  \n",
      " 9   DRIVING_FLAG                         233154 non-null  int64  \n",
      " 10  PASSPORT_FLAG                        233154 non-null  int64  \n",
      " 11  PERFORM_CNS_SCORE                    233154 non-null  float64\n",
      " 12  PERFORM_CNS_SCORE_DESCRIPTION        233154 non-null  object \n",
      " 13  NEW_ACCTS_IN_LAST_SIX_MONTHS         233154 non-null  float64\n",
      " 14  DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS  233154 non-null  float64\n",
      " 15  NO_OF_INQUIRIES                      233154 non-null  float64\n",
      " 16  LOAN_DEFAULT                         233154 non-null  int64  \n",
      " 17  AGE                                  233154 non-null  float64\n",
      " 18  DISBURSAL_MONTH                      233154 non-null  int64  \n",
      " 19  AVERAGE_ACCT_AGE_MONTH               233154 non-null  float64\n",
      " 20  CREDIT_HISTORY_LENGTH_MONTHS         233154 non-null  float64\n",
      " 21  DISBURSED_CAT                        233154 non-null  object \n",
      " 22  DISBURSAL_DIFFERENCE                 233154 non-null  float64\n",
      " 23  TOTAL_ACCTS                          233154 non-null  float64\n",
      " 24  TOTAL_ACTIVE_ACCTS                   233154 non-null  float64\n",
      " 25  TOTAL_OVERDUE_ACCTS                  233154 non-null  float64\n",
      " 26  TOTAL_CURRENT_BALANCE                233154 non-null  float64\n",
      " 27  TOTAL_SANCTIONED_AMOUNT              233154 non-null  float64\n",
      " 28  TOTAL_DISBURSED_AMOUNT               233154 non-null  float64\n",
      " 29  TOTAL_INSTAL_AMT                     233154 non-null  float64\n",
      " 30  OVERDUE_PCT                          233154 non-null  float64\n",
      "dtypes: float64(19), int64(9), object(3)\n",
      "memory usage: 56.9+ MB\n"
     ]
    }
   ],
   "source": [
    "#look at the columns\n",
    "loan_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important that our classifier recognises categorical variables where appropriate.\n",
    "\n",
    "Lets use the dtypes property to look at the variable types of our categorical feilds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DISBURSED_CAT                    object\n",
       "DISBURSAL_MONTH                   int64\n",
       "PERFORM_CNS_SCORE_DESCRIPTION    object\n",
       "STATE_ID                          int64\n",
       "MANUFACTURER_ID                   int64\n",
       "EMPLOYMENT_TYPE                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at categorical data types\n",
    "category_cols = ['DISBURSED_CAT', 'DISBURSAL_MONTH', 'PERFORM_CNS_SCORE_DESCRIPTION', 'STATE_ID', 'MANUFACTURER_ID', 'EMPLOYMENT_TYPE']\n",
    "loan_df[category_cols].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- We do not want to treat MANUFACTURER_ID, STATE_ID and DISBURSAL_MONTH as integers\n",
    "- We can encode our categorical columns with the category data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DISBURSED_CAT                    category\n",
       "DISBURSAL_MONTH                  category\n",
       "PERFORM_CNS_SCORE_DESCRIPTION    category\n",
       "STATE_ID                         category\n",
       "MANUFACTURER_ID                  category\n",
       "EMPLOYMENT_TYPE                  category\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert to categorical type\n",
    "loan_df[category_cols] = loan_df[category_cols].astype('category')\n",
    "loan_df[category_cols].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE \n",
    "\n",
    "- To keep our first model simple, select 6 variables including 'LOAN_DEFAULT' and 'DISBURSED_CAT'\n",
    "- Using these variables create a subset of loan_df and store it as a separate DataFrame loan_df_sml\n",
    "- HINT: Think about the results of your exploratory analysis, which variables might be good predictors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION\n",
    "\n",
    "- I have selected the following 6 columns, 'STATE_ID', 'LTV', 'DISBURSED_CAT', 'PERFORM_CNS_SCORE', 'DISBURSAL_MONTH', 'LOAN_DEFAULT'\n",
    "- You could have selected any five which you are interested in, so long as one of them is 'LOAN_DEFAULT' and you have 'DISBURSED_CAT' which we will use later in this chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type solution here\n",
    "small_cols = ['STATE_ID', 'LTV', 'DISBURSED_CAT', 'PERFORM_CNS_SCORE', 'DISBURSAL_MONTH', 'LOAN_DEFAULT']\n",
    "loan_df_sml = loan_df[small_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Let's have a quick look at our new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(233154, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the dimensions\n",
    "loan_df_sml.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 233154 rows but now there are only 6 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 233154 entries, 420825 to 630213\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count   Dtype   \n",
      "---  ------             --------------   -----   \n",
      " 0   STATE_ID           233154 non-null  category\n",
      " 1   LTV                233154 non-null  float64 \n",
      " 2   DISBURSED_CAT      233154 non-null  category\n",
      " 3   PERFORM_CNS_SCORE  233154 non-null  float64 \n",
      " 4   DISBURSAL_MONTH    233154 non-null  category\n",
      " 5   LOAN_DEFAULT       233154 non-null  int64   \n",
      "dtypes: category(3), float64(2), int64(1)\n",
      "memory usage: 7.8 MB\n"
     ]
    }
   ],
   "source": [
    "#look at the columns\n",
    "loan_df_sml.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Test Split\n",
    "\n",
    "- Before we fit (train) our basic linear model we need to split our data into training and test sets.\n",
    "- Training Data: used to fit the model to our specific data\n",
    "- Test Data: used to test the predictive power of the trained model  \n",
    "\n",
    "We can use the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from sklearn to create our training and test sets\n",
    "\n",
    "[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) has two required parameters \n",
    "\n",
    "- x: all of the rows and columns except the target variable \n",
    "- y: all of the rows but just the target variable column\n",
    "\n",
    "### EXERCISE\n",
    "\n",
    "- Create two variables x and y to match the required parameters for [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type solution here\n",
    "x = loan_df_sml.drop(['LOAN_DEFAULT'], axis = 1)\n",
    "y = loan_df_sml['LOAN_DEFAULT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should investigate the dimensions of x and y to make sure the above solution is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(233154, 5)\n",
      "(233154,)\n"
     ]
    }
   ],
   "source": [
    "#check the rows and columns\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 233154 entries, 420825 to 630213\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count   Dtype   \n",
      "---  ------             --------------   -----   \n",
      " 0   STATE_ID           233154 non-null  category\n",
      " 1   LTV                233154 non-null  float64 \n",
      " 2   DISBURSED_CAT      233154 non-null  category\n",
      " 3   PERFORM_CNS_SCORE  233154 non-null  float64 \n",
      " 4   DISBURSAL_MONTH    233154 non-null  category\n",
      "dtypes: category(3), float64(2)\n",
      "memory usage: 6.0 MB\n"
     ]
    }
   ],
   "source": [
    "#x info\n",
    "x.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y info\n",
    "y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Looks like we have what need, now we can create our training/test data sets \n",
    "\n",
    "In addition to the required parameters of [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) we will also use \n",
    "- test_size: floating value between 0 and 1 indicating the size of the test set \n",
    "- random_state: integer value used for random seeding, allows for repeatability of the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) returns 4 output values \n",
    "\n",
    "- x_train: the training rows without the target variable \n",
    "- x_test: the test rows without the target variable \n",
    "- y_train: the training rows, target variable only \n",
    "- y_test: the test rows, target variable only \n",
    "\n",
    "Let's familiarize ourselves with this output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train has 186523 rows and 5 columns\n",
      "x_test has 46631 rows and 5 columns\n",
      "y_train has 186523 rows\n",
      "y_test has 46631 rows\n"
     ]
    }
   ],
   "source": [
    "#check rows and columns\n",
    "print(\"x_train has {0} rows and {1} columns\".format(x_train.shape[0], x_train.shape[1]))\n",
    "print(\"x_test has {0} rows and {1} columns\".format(x_test.shape[0], x_test.shape[1]))\n",
    "print(\"y_train has {0} rows\".format(y_train.count()))\n",
    "print(\"y_test has {0} rows\".format(y_test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the number of rows and columns is what we would expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 186523 entries, 633275 to 501520\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count   Dtype   \n",
      "---  ------             --------------   -----   \n",
      " 0   STATE_ID           186523 non-null  category\n",
      " 1   LTV                186523 non-null  float64 \n",
      " 2   DISBURSED_CAT      186523 non-null  category\n",
      " 3   PERFORM_CNS_SCORE  186523 non-null  float64 \n",
      " 4   DISBURSAL_MONTH    186523 non-null  category\n",
      "dtypes: category(3), float64(2)\n",
      "memory usage: 4.8 MB\n"
     ]
    }
   ],
   "source": [
    "#x train info\n",
    "x_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNIQUEID\n",
       "633275    1\n",
       "646002    0\n",
       "591252    0\n",
       "475736    0\n",
       "639478    0\n",
       "Name: LOAN_DEFAULT, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y train info\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 46631 entries, 617183 to 626383\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype   \n",
      "---  ------             --------------  -----   \n",
      " 0   STATE_ID           46631 non-null  category\n",
      " 1   LTV                46631 non-null  float64 \n",
      " 2   DISBURSED_CAT      46631 non-null  category\n",
      " 3   PERFORM_CNS_SCORE  46631 non-null  float64 \n",
      " 4   DISBURSAL_MONTH    46631 non-null  category\n",
      "dtypes: category(3), float64(2)\n",
      "memory usage: 1.2 MB\n"
     ]
    }
   ],
   "source": [
    "#x test info\n",
    "x_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNIQUEID\n",
       "617183    1\n",
       "515702    0\n",
       "466872    0\n",
       "632384    0\n",
       "461426    0\n",
       "Name: LOAN_DEFAULT, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y test info\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brilliant! All the train and test data has the correct columns \n",
    "\n",
    "Now let's use [value_counts](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html) to check the distribution of the class variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.783099\n",
       "1    0.216901\n",
       "Name: LOAN_DEFAULT, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the training target variable\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.782248\n",
       "1    0.217752\n",
       "Name: LOAN_DEFAULT, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the test target variable\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Both the training and test set contain defaulted loans at 21.7%! \n",
    "\n",
    "We did not need to stratify due to the size of the dataset and the random nature of the sampling in [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 - Variable Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time build our first binary classifier!\n",
    "\n",
    "First we create the model object using [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model\n",
    "logistic_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we [fit](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit) the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '60k - 75k'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#fit the logistic model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mlogistic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1508\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1506\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1508\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1510\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1513\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1515\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1516\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 964\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric)\n\u001b[0;32m    981\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    750\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '60k - 75k'"
     ]
    }
   ],
   "source": [
    "#fit the logistic model\n",
    "logistic_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding \n",
    "\n",
    "Ok, looks like we made a mistake!\n",
    "\n",
    "The problem is that Logistic Regression, like most machine learning methods, does not know how to deal with string data\n",
    "\n",
    "We can use [pd.get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) to one hot encode our categorical variables \n",
    "\n",
    "- Remember in lesson 1 we converted our categorical variables to the 'category' data type \n",
    "- If we didn't do this, variables like STATE_ID which contained integer representations of categories would be missed by [pd.get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)\n",
    "- Then they would be incorrectly treated as continuous variables\n",
    "\n",
    "Lets one hot encode our small dataframe and assign it to a new variable 'loan_data_dumm'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode\n",
    "loan_data_dumm = pd.get_dummies(loan_df_sml, prefix_sep = '_', drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are passing three parameters to [pd.get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)\n",
    "\n",
    "- loan_df_sml: our small dataframe which we want to encode \n",
    "- prefix_sep: prefix separator for the dummy variables, new columns will be created like 'CNS_SCORE_CAT_Low'\n",
    "- drop_first: drop the first dummy variable for each category \n",
    "\n",
    "HOLD ON! Why are we dropping the first dummy variable for each category?\n",
    "- Think about it\n",
    "- If we have 10 boolean variables indicating the presence of some category and there are no missing values \n",
    "- Then if the variable doesn't belong to one of 9 of the categories \n",
    "- It must belong to the 10th \n",
    "- So we can drop one of the dummy columns without losing any information\n",
    "- This helps to simplify the model and reduce the impact of correlated variables \n",
    "\n",
    "Let's look at the results of [pd.get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 233154 entries, 420825 to 630213\n",
      "Data columns (total 40 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   LTV                       233154 non-null  float64\n",
      " 1   PERFORM_CNS_SCORE         233154 non-null  float64\n",
      " 2   LOAN_DEFAULT              233154 non-null  int64  \n",
      " 3   STATE_ID_2                233154 non-null  uint8  \n",
      " 4   STATE_ID_3                233154 non-null  uint8  \n",
      " 5   STATE_ID_4                233154 non-null  uint8  \n",
      " 6   STATE_ID_5                233154 non-null  uint8  \n",
      " 7   STATE_ID_6                233154 non-null  uint8  \n",
      " 8   STATE_ID_7                233154 non-null  uint8  \n",
      " 9   STATE_ID_8                233154 non-null  uint8  \n",
      " 10  STATE_ID_9                233154 non-null  uint8  \n",
      " 11  STATE_ID_10               233154 non-null  uint8  \n",
      " 12  STATE_ID_11               233154 non-null  uint8  \n",
      " 13  STATE_ID_12               233154 non-null  uint8  \n",
      " 14  STATE_ID_13               233154 non-null  uint8  \n",
      " 15  STATE_ID_14               233154 non-null  uint8  \n",
      " 16  STATE_ID_15               233154 non-null  uint8  \n",
      " 17  STATE_ID_16               233154 non-null  uint8  \n",
      " 18  STATE_ID_17               233154 non-null  uint8  \n",
      " 19  STATE_ID_18               233154 non-null  uint8  \n",
      " 20  STATE_ID_19               233154 non-null  uint8  \n",
      " 21  STATE_ID_20               233154 non-null  uint8  \n",
      " 22  STATE_ID_21               233154 non-null  uint8  \n",
      " 23  STATE_ID_22               233154 non-null  uint8  \n",
      " 24  DISBURSED_CAT_150k - 1m   233154 non-null  uint8  \n",
      " 25  DISBURSED_CAT_30k - 45k   233154 non-null  uint8  \n",
      " 26  DISBURSED_CAT_45k - 60k   233154 non-null  uint8  \n",
      " 27  DISBURSED_CAT_60k - 75k   233154 non-null  uint8  \n",
      " 28  DISBURSED_CAT_75k - 150k  233154 non-null  uint8  \n",
      " 29  DISBURSAL_MONTH_2         233154 non-null  uint8  \n",
      " 30  DISBURSAL_MONTH_3         233154 non-null  uint8  \n",
      " 31  DISBURSAL_MONTH_4         233154 non-null  uint8  \n",
      " 32  DISBURSAL_MONTH_5         233154 non-null  uint8  \n",
      " 33  DISBURSAL_MONTH_6         233154 non-null  uint8  \n",
      " 34  DISBURSAL_MONTH_7         233154 non-null  uint8  \n",
      " 35  DISBURSAL_MONTH_8         233154 non-null  uint8  \n",
      " 36  DISBURSAL_MONTH_9         233154 non-null  uint8  \n",
      " 37  DISBURSAL_MONTH_10        233154 non-null  uint8  \n",
      " 38  DISBURSAL_MONTH_11        233154 non-null  uint8  \n",
      " 39  DISBURSAL_MONTH_12        233154 non-null  uint8  \n",
      "dtypes: float64(2), int64(1), uint8(37)\n",
      "memory usage: 15.3 MB\n"
     ]
    }
   ],
   "source": [
    "#check the columns\n",
    "loan_data_dumm.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Looks like we have dummy columns for our categoricals\n",
    "\n",
    "### EXERCISE \n",
    "\n",
    "- Take time to investigate the contents these new columns \n",
    "- Make sure you understand how [pd.get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) is transforming our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra space for exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 - Train and Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE \n",
    "\n",
    "- Recreate our training and test set using loan_data_dumm\n",
    "- Make sure the class distributions are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.782975\n",
      "1    0.217025\n",
      "Name: LOAN_DEFAULT, dtype: float64\n",
      "0    0.782821\n",
      "1    0.217179\n",
      "Name: LOAN_DEFAULT, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#type solution here\n",
    "x = loan_data_dumm.drop(['LOAN_DEFAULT'], axis = 1)\n",
    "y = loan_data_dumm['LOAN_DEFAULT']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "print(y_train.value_counts(normalize = True))\n",
    "print(y_test.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to [fit](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit) our model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richard.rodrot\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#intialize and train logistic regression\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! We are nearly there. We have successfully trained our model. But there is a warning we should take care of.\n",
    "\n",
    "The above warning is telling us that the LogisticRegression did not find a solution to fit our data within the maximum number of iterations. \n",
    "\n",
    "The specifics of this error are out of the scope of this course but the likely explanations are that our data is not actually linearly separable or that our selected columns and pre-processing do not provide enough information to make distinct separations on the data. \n",
    "\n",
    "Something to keep in mind, but for now, we can try to resolve the warning by increasing the maximum allowed iterations.\n",
    "\n",
    "The default value is 100, so let's try 200!\n",
    "\n",
    "*The waring may or may not appear depending on your system, if you don't see any problems you can skip this step*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=200)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model\n",
    "logistic_model = LogisticRegression(max_iter=200)\n",
    "logistic_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have successfully trained our model\n",
    "\n",
    "Now we need to generate some predictions for our test set\n",
    "\n",
    "We pass our test features to the model to generate predictions, using [predict](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate predictions\n",
    "preds = logistic_model.predict(x_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of predict is an array of 0s and 1s representing the loan default prediction\n",
    "\n",
    "This is great but we need some measure of model performance\n",
    "\n",
    "The [score](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.score) function generates predictions and compares the predicted class with the actual class. The output is a floating-point number between 0 and 1 telling us the percentage of loans we correctly classified!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7828212789683617"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get accuracy\n",
    "logistic_model.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Looks like our model performed quite well, it predicted 78% of our test cases correctly.\n",
    "\n",
    "Don't get too excited, accuracy can be a misleading measure of model performance! The next chapter will look at other measures of model performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
